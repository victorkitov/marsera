# MARS
*(Jerome H. Friedman 1991)*

## 2. Existing methodology

#### 2.1. Global parametric modeling
![[Pasted image 20221027204321.png]]
***
#### 2.2. Nonparametric modeling
![[Pasted image 20221027205111.png]]
#### Local parametric approximations (smoothers)
![[Pasted image 20221027205933.png]]
***
#### 2.3. Low dimensional expansions
![[Pasted image 20221027230535.png]]
![[Pasted image 20221027230624.png]]
***
#### 2.4.1 Projecticn pursuit regression
![[Pasted image 20221027235818.png]]
![[Pasted image 20221028000003.png]]
***
#### 2.4.2 Recursive partitioning regression (RPR)
![[Pasted image 20221029151454.png]]
![[Pasted image 20221029181421.png]]
![[Pasted image 20221030004559.png]]
***
***


## 3. Adaptive regression splines

#### 3.1. Recursive partitioning regression revisited
**MARS** - это многомерная непараметрическая (чем больше данных, тем лучше работает) регрессия (обобщение **Recursive partitioning regression (RPR)**).
***
![[Pasted image 20221029201615.png]]
$LOF(g)$ - процедура, вычисляющая несоответствие ф-ции $g(x)$ данным.
![[Pasted image 20221029202129.png]]

Находим одну координату $\mathbf{x}$ и одно значение этой координаты на всех эл-ах обуч. выборки для одной из имеющихся базисных функций, такие что $LOF(g)$, оптимизированная по множителям $a_1, ..., a_M$, (задача лин. регрессии) оказывается с самым маленьким значением. В качестве новых б. ф. выступают старая ф-ция, домноженная на $H$ с противоположными по знаку аргументами.

![[Pasted image 20221030005227.png]]
![[Pasted image 20221030005637.png]]
Сначала генерируется большое кол-во б. ф., затем происходит процедура удаления тех ф-ций, которые не вносят ощутимый вклад в качество модели. Просто удалять ф-ции не получится, т.к. тогда образуются "дыры".
***
#### 3.2. Continuity
Проблема **RPR** - отсутствие непрерывности на границах субрегионов. Можно модифицировать алг. 1, но для этого также необходимо обобщить кусочно-постоянную ф-цию  $H$ непрерывным аналогом (кус-непр ф-ция - частный случай сплайновых б.ф.).

**Сплайн** — [функция](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0) "Функция (математика)") в [математике](https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0 "Математика"), область определения которой разбита на конечное число отрезков, на каждом из которых она совпадает с некоторым алгебраическим [многочленом](https://ru.wikipedia.org/wiki/%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D1%87%D0%BB%D0%B5%D0%BD "Многочлен") ([полиномом](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%BB%D0%B8%D0%BD%D0%BE%D0%BC "Полином")). Максимальная из степеней использованных полиномов называется **степенью сплайна**. Разность между степенью сплайна и получившейся [гладкостью](https://ru.wikipedia.org/wiki/%D0%93%D0%BB%D0%B0%D0%B4%D0%BA%D0%B0%D1%8F_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F "Гладкая функция") называется **дефектом сплайна**.

![[Pasted image 20221030014622.png]]
![[Pasted image 20221104011735.png]]
![[Pasted image 20221030112625.png]]
Но так появляется много других проблем, которые надо как-то решить.
***
#### 3.3. A further generalization
*Требуется: найти небольшой (чтобы дисперсия была маленькой) набор б.ф., который хорошо описывает исходную зав-ть (чтобы смещение было маленьким).*

Ещё одна проблема **RPR** - неспособность хорошо приближать достаточно простые ф-ции (в частности линейные и аддитивные), зависящие от небольшого числа переменных.

*Геометрическая интерпретация проблемы:*
Необходимо очень много гиперпрямоугольников, ориентированных по осям, чтобы выучить функциональную зависимость ф-ций, которые в каждом прямоугольнике ориентированы под углом к осям.

*Почему возниакет проблема?*
Использование кус.-пост. ф-ций.

*Ещё одна проблема:*
Б.ф. обычно зависят от большого числа переменных  (т.к. родительская ф-ция, зависящая от $n$ переменных, в общем случае заменяется на ф-цию, зависящую от $n + 1$ переменной), поэтому ф-ции, зависящие от малого числа переменных, плохо аппроксимируются.

*Идея:*
Не удалять родительскую б.ф. (т.е. каждый раз кол-во б.ф. увеличивается на $2$). Т.о. б.ф. $B_1(x) = 1$ также включается, что позволяет создавать аддитивные модели и, в частности, линейные.

Также решается проблема многократного разделения по одной и той же переменной. Т.к. иначе результирующий базис не сведётся к набору нужных сплайновых б.ф. (степень при переменной не будет соответствовать степени сплайна). Такое разделение происходило раньше из-за невозможности адекватной аппроксимации аддитивных ф-ций, но теперь эта проблема решена.

*Итоговые модификации алгоритма 1:*
1. Замена ступенчатой ф-ции на усеченную степенную сплайновую
ф-цию.
2. Не удалять родительскую б.ф. $B_{m^*}(\textbf{x})$ после ее разделения.
3. В каждом произведении, соотв. б.ф., множители должны зависеть от разных переменных.

Важно также выбрать степень непрерывности итоговой аппрокимации, которая задаётся параметром $q$.
***
#### 3.4. MARS algorithm
Модифицированный алгоритм 1.
![[Pasted image 20221030155511.png]]

Обратный шаг - прореживание набора б.ф. Жадный алгоритм.
На первом шаге удаляем из исходного мн-ва по одной б.ф. Для получившихся мн-в считаем $LOF$.
* Если он улучшился локально => перезаписываем лучшее локальное значение $LOF$ $b$ и лучшее локальное мн-во  $K^*$.
* Если он улучшился глобально => перезаписываем лучшее глобальное зн-е $LOF$ $lof^*$ и лучшее глобальное мн-во $J^*$.

На следующем шаге в качестве мн-ва, из которго будем удалять б.ф., берём лучшее локальное мн-во $K^*$ с предыдущего шага и повторяем процедуру, пока не останется ==одна== константная б.ф. $B_1(x) = 1$, которая должна оставаться в любом мн-ве. В итоге получается вложенная посл-ть моделей с уменьшающимся кол-вом б.ф.

![[Pasted image 20221030161740.png]]
***
#### 3.5. ANOVA decomposition
Приближение целевой ф-ции после проделанных шагов:

![[Pasted image 20221030184603.png]]

Перепишем аппроксимирующую ф-цию в более информативном виде. Для этого сгруппируем б.ф., зависящие от одних и тех же переменных:

 ![[Pasted image 20221030184626.png]]
![[Pasted image 20221030185158.png]]
![[Pasted image 20221030185608.png]]
![[Pasted image 20221030185938.png]]
*Кажется, что в 24 ошибка: сумма нужна не по $m$, а по $i$.*
Данное разложение наз-ся разложением **ANOVA (Analysis of variance)** - анализ дисперсии, часто используется в статистике.
Это разложение более интерпретируемо. По нему сразу видно, какие переменные и в каком функциональном виде входят в модель. При этом есть возможность построения графиков по переменным или их проекциям.
***
#### 3.6. Model selection
Осталось ещё определить критерий несоответствия $LOF$ и максимальное кол-во б.ф. $M_{max}$. 

$LOF:$
Сначала определим ф-цию расстояния:
![[Pasted image 20221031005621.png]]
Использование квадратичной ф-ции объясняется некоторыми вычислительными упрощениями.

**Generalized cross-validation criterion:**
*(Craven and Wahba (1979))*
![[Pasted image 20221031010722.png]]
Штраф в знаменатале нужен для учёта увеличивающейся дисперсии из-за увеличения кол-ва б.ф.

![[Pasted image 20221031010927.png]]

Где смысл $C(M)$ - число л.н. б.ф. (не знаю почему так, в статье выше ответ)

*(Friedman and Silverman (1989))*

![[Pasted image 20221102212923.png]]

Где: $d$ - параметр сглаживания (чем больше, тем меньше узлов создаётся). Обычно выбирается из диапазона $2 \le d \le 4$ (стандартный вариант $d = 3$). **GCV** зависит от $d$, а  **MSE** не зависит.

*TODO: Добавить в код простое вычисление $C(M)$ как в книге Statistical learning.*

$M_{max}:$
Обычно $M_{max} = 2M^*$ где $M^*$ - оптимальное кол-во б.ф. (в смысле $GCV$)

#### 3.7. Degree-of-continuity
Иметь первую производную полезно для внешнего вида ф-ции, для увеличения доступных методов оптимизации таких ф-ций и т.д. Для этого необходимо иметь как минимум 2ю степень у сплайновых усечённых б.ф.

Ещё одна проблема - граничные эффекты, вызванные (если я правильно понял) резким переходом в точке излома.

Авторы предлагают вместо:
![[Pasted image 20221105015210.png]]
использовать:
![[Pasted image 20221105015235.png]]

Теперь это уже не линейные, а кубические усечённые сплайны с непрерывными 1ми производными, повторяющие вдали от излома hinge-сплайны.

![[Pasted image 20221105015437.png]]
Кусочно-линейные сплайны задавались одной точкой излома $t$. Данные кубические сплайны задаются той же центральной точкой излома $t$ и ещё 2мя точками $t_-, t_+$ - средние точки между соседними центральными. Так сделано для уменьшения разрывов 2ой производной, которая и так разрывна в центральных точках *(TODO: проверить)*:


![[Pasted image 20221105021537.png]]

#### 3.8. Knot optimization
Для **ANOVA** разложения верна следующая форма записи:
![[Pasted image 20221105152752.png]]

Тут второе слагаемое в (36) - это вклад набора переменных $V(m)$ и $V(m) + \{v\}$, (37) - вклады всех остальных наборов переменных. Оптимизация в алгоритме происходит по набору параметрам $\{c_j\}$ и по набору параметров $\{a_i\}$.

![[Pasted image 20221105154909.png]]
$R_{mv}$ - остаток, который должен быть приближен $\phi_{mv}$.
Т.е. исходную задачу аппроксимации моделью исходной ф-ции мы свели к задаче аппроксимации остатка, который должен быть приближен в среднем по всем б.ф. и по всем координатам (матожидаем по этим параметрам).
Также, теперь в задаче явно не фигурирует многомерный вектор $x$, а только его отдельные координаты. *(чего?)*

Как видно, в (40) зависимость только от одной переменной *(это если не матожидать)*. Если зафиксировать значения $\{a_i\}$, то (40) имеет общее решение (*почему это общее решение? как его получили?*):

![[Pasted image 20221105155032.png]]

*(Всё далее не претендует на правду)*
Это может быть оценено с помощью взвешенного сглаживания. Если придать сглаживанию форму кусочно-линейного сплайнового приближения, то мы косвенно приходим к **MARS**. Тут есть связь с методом сглаживания **TURBO**. Многие шаги очень похожи (например, выбор $t$).
Если разрешать в качестве порога разбиения $t$ использовать любую точку из обучающих данных, то, в рамках задачи сглаживания, допускается сглаживание по одному наблюдению, что недопустимо, т.к. шум будет сильно влиять. Поэтому хочется найти минимальный интервал (число $L(\alpha)$ данных между каждым узлом), устойчивый к шумам.

![[Pasted image 20221106014639.png]]
$Pr(L^*)$ - вероятность $\ge L^*$ успехов в $nN_m$ испытаниях Бернулли с $p = 0.5$,
$\alpha$ - маленькое число (0.05 или 0.01),
$N_m$ - кол-во наблюдений, для которых $B_m > 0$
*(не понял, в чём смысл $nN_m$)*
$n$ - размерность объекта

*Идея такая:*
Пусть есть некоторый прямоугольник, в котором допускается присутствие $nN_m$ точек. Т.к. наши данные - это истинная ф-ция + некоторый шум со средним в нуле (пусть будет так). Т.о. этот шум в нетривиальном случае обязан иметь чередующиеся знаки. Будем считать, что значение шума есть $+1, -1$. Тогда, мы адекватно приближаем истинную зависимость, если шум в каждой наблюдаемой точке не сделался очень большим с точностью до знака.

![[Pasted image 20221106013309.png]]
![[Pasted image 20221106013323.png]]
$Le(\alpha)$ - аналог  $L(\alpha)$ только уже для внешних порогов, а не внутренних.

#### 3.9. Computational considerations.
Решать задачу **МНК** можно разными способами:
* используя $QR$-разложение. Такой способ хорош численно;
* используя разложение Холецкого $L^TL$ (для симметр. полож. опред. матрицы), который численно менее устойчивый.

*В (46) ошибка - перепутаны размерности.*
![[Pasted image 20221106162530.png]]
![[Pasted image 20221106154238.png]]
![[Pasted image 20221106154410.png]]
Матрица $V$ - как раз симметричная и положительно определённая матрица.

*Замечание:*
При формировании $V$ и $c$ производится нормализация (вычитание среднего). Т.о. $V$ в некотором смысле аналогична ковариационной матрице для $B$ *(так написано в статье)*, однако, в $V$ нормализуется только один множитель. *(Возможно, это нужно для упрощения формул пересчёта, чтобы там не возникали страшные слагаемые - предположение Влада)* *(реш-ие (48) эквивалентно (49) - проверил, полную нормализацию начал проверять, но не получилось сразу, поэтому нужно допроверить)*

*Проблема:*
Необходимо много раз считать значения новых двух б.ф. на всех объектах обучающей выборки, а далее решать задачу оптимизации для поиска соответствующего порога $t$ (проход по порогам самый долгий, т.к. необходимо пройти по всем объектам обучающей выборки $N$). Поэтому хотим как угодно, но упростить задачу.

Итоговая сложность **MARS**a "в-тупую":
![[Pasted image 20221106171403.png]]
*(Полностью разобраться в асимптотиках)*

*Идея:*
Для уменьшения вычислений вместо ф-ции $g$ используется ф-ция $g^{'}$, оптимизация которой приводит к тем же $lof^*$ и $t^*$, но значения $\{a\}_i$ будут другими. Преимущество такой ф-ции заключается в том, что при изменении $t$ изменяется только одна б.ф.
![[Pasted image 20221106171643.png]]
Если отсортировать возможные пороги и координаты, то не трудно получить пересчётные формулы. По ним в целой матрице $V$, которая явл-ся симметричной и положительно определённой, достаточно пересчитать только последнюю строку (он же последний столбец), а для вектора $c$ - только последний элемент $c_{M+1}$. Оптимальный вектор коэффициентов $a$ находится из системы *(48)*, в которой матрица $V$ раскладывается по методу Холецкого в  $L^TL$, где $L$ - нижнетреугольная матрица. Т.о. оптимизация по $a$ и нахождение $lof$ сводится к разложению матрицы $V$ и реш-ю 2х СЛАУ с треугольными матрицами.
![[Pasted image 20221106172439.png]]
![[Pasted image 20221225222858.png]]

*Выигрыш:*
Теперь вместо того, чтобы проходиться по всем порогам $t$ (эл-ам обуч. выборки), пересчитывая для них новые б.ф. $B_M(x)$ и $B_{M+1}(x)$ ($2N$ операций для обновления матрицы $B$), а потом перемножая $B^T$ и $B$ для получения матрицы $V$ и аналогично для получения вектора $c$, мы  сразу пересчитываем последний столбец матрицы $V$ и эл-т вектора $c$.

Итоговая сложность оптимизированного **MARS**a:
*TODO: привести конечную асимптотику*

## 4. Эксперименты
##### 4.1 *Может ли сложиться такая ситуация?*
$B_1(x)=1$
$B_2(x)=[+(x_i-t)]_+$
$B_3(x)=[-(x_i-t)]_+$
$B_4(x) = [+(x_i-u)]_+$
$B_5(x)=[-(x_i-u)]_+$

Т.е. создались две б.ф. с 1ой координатой $x_i$. а сразу же после создались ещё две б.ф. с той же координатой, но с другим порогом.

*Идеи:* скорее всего может быть, т.к. иначе следствие из ANOVA нетривиального смысла.

##### 4.2 Модельные датасеты

##### 4.3Популярные датасеты из sklearn
###### 4.3.1 Бостон
