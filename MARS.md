*(Jerome H. Friedman 1991)*

## 2. Existing methodology

#### 2.1. Global parametric modeling
![[Pasted image 20221027204321.png]]
***
#### 2.2. Nonparametric modeling
![[Pasted image 20221027205111.png]]
#### Local parametric approximations (smoothers)
![[Pasted image 20221027205933.png]]
***
#### 2.3. Low dimensional expansions
![[Pasted image 20221027230535.png]]
![[Pasted image 20221027230624.png]]
***
#### 2.4.1 Projecticn pursuit regression
![[Pasted image 20221027235818.png]]
![[Pasted image 20221028000003.png]]
***
#### 2.4.2 Recursive partitioning regression (RPR)
![[Pasted image 20221029151454.png]]
![[Pasted image 20221029181421.png]]
![[Pasted image 20221030004559.png]]
***
***


## 3. Adaptive regression splines

#### 3.1. Recursive partitioning regression revisited
**MARS** - это многомерная непараметрическая регрессия, которая есть обощение **Recursive partitioning regression (RPR)**.
***
*Идея:* вместо геометрического понятия областей и расщепления - арифметические понятия сложения и умножения.
![[Pasted image 20221029201615.png]]
![[Pasted image 20221029201623.png]]
![[Pasted image 20221029202003.png]]
$LOF(g)$ - процедура, вычисляющая несоответствие ф-ции $g(x)$ данным.
![[Pasted image 20221029202129.png]]

Находим одну координату $\mathbf{x}$ и одно значение этой координаты на всех эл-ах обуч. выборки для одной из имеющихся базисных функций, такие что $LOF(g)$, оптимизированная по множителям $a_1, ..., a_M$, (задача лин. регрессии) оказывается с самым маленьким значением. В качестве новых базисных функций выступают старая ф-ция, домноженная на $H$ с противоположными по знаку аргументами.

![[Pasted image 20221030005227.png]]
![[Pasted image 20221030005637.png]]
Сначала генерируется большое кол-во базисных ф-ций, затем происходит процедура удаления тех ф-ций, которые не вносят ощутимый вклад в качество модели. Просто удалять ф-ции не получится, т.к. тогда образуются "дыры".
***
#### 3.2. Continuity
Проблема **RPR** - отсутствие непрерывности на границах субрегионов. Можно модифицировать алг. 1, но для этого также необходимо обобщить кусочно-постоянную ф-цию  $H$ непрерывным аналогом (кус-непр ф-ция - частный случай сплайновых базисных ф-ций).

**Сплайн** — [функция](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0) "Функция (математика)") в [математике](https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0 "Математика"), область определения которой разбита на конечное число отрезков, на каждом из которых она совпадает с некоторым алгебраическим [многочленом](https://ru.wikipedia.org/wiki/%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D1%87%D0%BB%D0%B5%D0%BD "Многочлен") ([полиномом](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%BB%D0%B8%D0%BD%D0%BE%D0%BC "Полином")). Максимальная из степеней использованных полиномов называется **степенью сплайна**. Разность между степенью сплайна и получившейся [гладкостью](https://ru.wikipedia.org/wiki/%D0%93%D0%BB%D0%B0%D0%B4%D0%BA%D0%B0%D1%8F_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F "Гладкая функция") называется **дефектом сплайна**.

![[Pasted image 20221030014622.png]]
![[Pasted image 20221104011735.png]]
![[Pasted image 20221030112625.png]]
Но так появляется много других проблем, которые надо как-то решить.
***
#### 3.3. A further generalization
*Требуется: найти небольшой (чтобы дисперсия была маленькой) набор б.ф., который хорошо описывает исходную зав-ть (чтобы смещение было маленьким).*

Ещё одна проблема **RPR** - неспособность хорошо приближать достаточно простые ф-ции (в частности линейные и аддитивные), зависящие от небольшого числа переменных.

*Геометрическая интерпретация проблемы:*
Необходимо очень много гиперпрямоугольников, ориентированных по осям, чтобы выучить функциональную зависимость ф-ций, которые в каждом таком прямоугольнике ориентированы под углом к этим осям.

*Почему возниакет проблема?* 
Б.ф. обычно зависят от большого числа переменных  (т.к. родительская ф-ция, зависящая от $n$ переменных, в общем случае заменяется на ф-цию, зависящую от $n + 1$ переменной), поэтому ф-ции, зависящие от малого числа, плохо аппроксимируются.

*Идея:*
Не удалять родительскую б.ф. (т.е. каждый раз кол-во б.ф. увеличивается на 2). Т.о. б.ф. $B_1(x) = 1$ также включается, что позволяет создавать аддитивные модели и, в частности, линейные.

Также решается проблема многократного разделения по одной и той же переменной. Так как иначе результирующий базис не сведётся к набору сплайновых б.ф. (степень при переменной не будет соответствовать степени сплайна). Такое разделение происходило раньше из-за невозможности адекватной аппроксимации аддитивных ф-ций, но теперь эта проблема решена (не знаю почему?).

*Итоговые модификации алгоритма 1:*
1. Замена ступенчатой функции на усеченную степенную сплайновую
функцию.
2. Не удалять родительскую базовую функцию $B_{m^*}(\textbf{x})$ после ее разделения.
3. В каждом произведении, соотв. б.ф., множители должны зависеть от разных переменных.

Важно также выбрать степень непрерывности итоговой аппрокимации, которая задаётся $q$.
***
#### 3.4. MARS algorithm
![[Pasted image 20221030155511.png]]

Жадный алгоритм. На первом шаге удаляем из всех б.ф. сначала одну ф-цию, перезаписываем лучшее мн-во в случае улучшения первоначального качества.  Затем выполняем такую же процедуру с множеством б.ф. без одной б.ф., которая показала наименьшее ухудшение качества (или наибольшее улучшение) на предыдущем шаге и т.д. При этом отслеживаем локально лучшее мн-во (в надежде, что через несколько шагов получится глобально лучшее мн-во).

*Может ли случиться так, что алгоритм будет всё время генерировать одинаковые одномерные произведения б.ф.?*

*Может ли быть так, что алгоритм будет строить одномерные произведения б.ф. по одной и той же переменной?

*Может ли быть так, что алгоритм будет строить подряд одномерные произведения б.ф.? )*

![[Pasted image 20221030161740.png]]

$B_1(x) = 1$ никогда не удаляется! В итоге получается посл-ть моделей с уменьшающимся кол-вом б.ф.
***
#### 3.5. ANOVA decomposition
Приближение целевой ф-ции после проделанных шагов:

![[Pasted image 20221030184603.png]]

Перепишем аппроксимирующую ф-цию в более информативном виде. Для этого сгруппируем б.ф., зависящие от одних и тех же переменных:

 ![[Pasted image 20221030184626.png]]
![[Pasted image 20221030185158.png]]
![[Pasted image 20221030185608.png]]
![[Pasted image 20221030185938.png]]
![[Pasted image 20221030190041.png]]
Данное разложение наз-ся разложением **ANOVA**
Это разложение гораздо более интерпретируемо. По нему сразу видно, какие переменные и в каком функциональном виде входят в модель. При этом есть возможность построения графиков по переменным или их проекциям.
***
#### 3.6. Model selection
$LOF:$
Осталось ещё определить критерий несоответствия $LOF$ и максимальное кол-во б.ф. $M_{max}$. Сначала определим ф-цию расстояния:
![[Pasted image 20221031005621.png]]
Использование **SE** объясняется некоторыми вычислительными упрощениями.

**Generalized cross-validation criterion:**
*(Craven and Wahba (1979))*
![[Pasted image 20221031010722.png]]
Штраф в знаменатале нужен для учёта увеличенной дисперсии из-за увеличения кол-ва б.ф.

![[Pasted image 20221031010927.png]]

Где: $C(M)$ - число л.н. б.ф. (не знаю почему так, в статье выше ответ)

*(Friedman and Silverman (1989))*

![[Pasted image 20221102212923.png]]

Где: $d$ - параметр сглаживания (чем больше, тем меньше узлов создаётся). Обычно выбирается из диапазона $2 \le d \le 4$ (стандартный вариант $d = 3$). **GCV** зависит от $d$, а  **MSE** почти не зависит.

$M_{max}:$
Обычно $M_{max} = 2M^*$ где $M^*$ - оптимальное кол-во б.ф. (в смысле $GCV$)

#### 3.7. Degree-of-continuity
Иметь первую производную полезно для внешнего вида ф-ции, для увеличения доступных методов оптимизации таких ф-ций и т.д. Для этого необходимо иметь как минимум 2ю степень у сплайновых усечённых б.ф.

Ещё одна проблема - граничные эффекты, вызванные (если я правильно понял) резким переходом в точке излома. (Но я до конца не понял почему)

Авторы предлагают вместо:
![[Pasted image 20221105015210.png]]
использовать:
![[Pasted image 20221105015235.png]]

Теперь это уже не линейные, а кубические усечённые сплайны с непрерывными 1ми производными, повторяющие вдали от излома hinge-сплайны.

![[Pasted image 20221105015437.png]]
Кусочно-линейные сплайны задавались одной точкой излома $t$. Данные кубические сплайны задаются той же центральной точкой излома $t$ и ещё 2мя точками $t_-, t_+$ - средние точки между соседними центральными. Так сделано для уменьшения разрывов 2ой производной, которая и так разрывна в центральных точках:


![[Pasted image 20221105021537.png]]

#### 3.8. Knot optimization
Для **ANOVA** разложения верна следующая форма записи:
![[Pasted image 20221105152752.png]]

Тут второе слагаемое в (36) - это вклад набора переменных $V(m)$ и $V(m) + {v}$, (37) - вклады всех остальных наборов переменных. Оптимизация в алгоритме происходит по набору параметрам $\{c_i\}$ и по набору параметров $\{a_i\}$.

*(Не знаю почему такое решение и почему это вообще названо сглаживанием, если стоит мат. ожидание)*

![[Pasted image 20221105154909.png]]

Как видно, в (40) зависимость только от одной переменной. Если зафиксировать значения $\{a_i\}$, то (40) имеет общее решение:

![[Pasted image 20221105155032.png]]

*(Всё далее не претендует на правду. Я так понял. Сорян)*
Это может быть оценено с помощью взвешенного сглаживания. Если придать сглаживанию форму кусочно-линейного сплайнового приближения, то мы косвенно приходим к **MARS**. Тут есть связь с методом сглаживания **TURBO**. Многие шаги очень похожи (например, выбор $t$).
Если разрешать в качестве порога разбиения $t$ использовать любую точку из обучающих данных, то, в рамках задачи сглаживания, допускается сглаживание по одному наблюдению, что недопустимо, т.к. шум будет иметь сильное влияние. Поэтому хочется найти минимальный интервал (число $L(\alpha)$ данных между каждым узлом), устойчивый к шумам.

![[Pasted image 20221106014639.png]]
$Pr(L^*)$ - вероятность $\ge L^*$ успехов в $nN_m$ испытаниях Бернулли с $p = 0.5$,
$\alpha$ - маленькое число (0.05 или 0.01),
$N_m$ - кол-во наблюдений, для которых $B_m > 0$
*(не понял, в чём смысл $nN_m$)*

Идея такая: есть некоторый пусть прямоугольник, в котором допускается присутствие $nN_m$ точек. Т.к. наши данные - это истинная ф-ция + некоторый шум со средним в нуле (пусть будет так). Т.о. этот шум в нетривиальном случае обязан иметь чередующиеся знаки. Будем считать, что значение шума есть $+1, -1$. Тогда, мы адекватно приближаем истинную зависимость, если шум в каждой наблюдаемой точке не сделался очень большим с точностью до знака. А почему так? Кто его знает

![[Pasted image 20221106013309.png]]
![[Pasted image 20221106013323.png]]
$Le(\alpha)$ - аналог  $L(\alpha)$ только уже для внешних порогов, а не внутренних.

#### 3.9. Computational considerations.
*Мотивировка:*
**MARS** - расширение **RPR**, который является основой методов **AID** и **CART**. Последние в свою очередь обладают хорошими показателями скорости обучения. Это происходит из-за использования ступенчатых ф-ций (не сильно понял почему, говорят, что два последних цикла в алгоритме 2 сильно упрощаются). Также, применяя **МНК** для подгонки, можно использовать очень простые формулы обновления весов, которые уменьшают вычислительную сложность с $O(NM^2 + M^3)$ (почти наверное, $M^3$ - обращение матрицы $B^TB$, $NM^2$ - перемножение матриц ) до $O(1)$. Итоговая сложность: $O(nNM_{max})$ (после какой-то сортировки). С **MARS** точно так не прокатит. Необходимо предложить аналогичный способ для увеличения эффективности.

Решать задачу подгонки **МНК** можно разными способами:
* используя $QR$-разложение. Такой способ даёт хорошие численные свойства.
* используя разложение Холецкого, который численно менее стабильный. Поэтому надо подходить к решению с особой осторожностью.
*В (46) кажется ошибка, перепутаны размерности.*
![[Pasted image 20221106162530.png]]
![[Pasted image 20221106154238.png]]

*(не сильно вникал)*
![[Pasted image 20221106154410.png]]

Итоговая сложность **MARS** методов "в-тупую" (прикидывал вроде также получилось)
![[Pasted image 20221106171403.png]]

Для уменьшения вычислений вместо ф-ции $g$ использовать ф-цию $g^{'}$, оптимизация которой приводит к тем же $lof^*$ и $t^*$. Значения $\{a\}_i$ будут другими. Преимуществом такой ф-ции заключается в том, что при изменении $t$ изменяется только одна б.ф.
![[Pasted image 20221106171643.png]]

Пересчётные формулы (не разобрался).
![[Pasted image 20221106172439.png]]

## 5. Remarks
### 5.1. Constraints
### 5.2. Semiparametric modeling
### 5.3. Collinearity
### 5.4. Robustness
